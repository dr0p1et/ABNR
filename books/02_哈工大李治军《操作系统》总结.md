# 哈工大李治军《操作系统》总结

## 0x01 读后感

终于完整刷完了李治军老师的《操作系统》视频课程。课程质量非常高，个人感觉比清华大学陈瑜老师的的《操作系统》课要生动并且更容易理解，非常适合初学者。

这个课最大的特点就是：老师从计算机的上电启动开始讲起，一步步引导提出问题，然后讨论怎么解决出现的问题。让学生在慢慢思考的过程中，逐渐弄明白如何从头设计一个操作系统。从如何使用计算机开始到如何高效地使用计算机，很自然地引出了CPU管理、内存管理、系统接口、设备管理、磁盘管理等内容。对于每个问题都采用循序渐进的方式，先提出最简单直接的解决方法，再根据方法中出现的问题一步步提出更优化的算法。每个模块的最后，都对照Linux0.1x的代码分析了具体的实现过程。

课程脉络：系统启动3讲，系统接口2讲，CPU管理12讲，内存管理6讲，设备管理2讲，文件系统5讲。

收获：

学完整套课程，基本对操作系统的实现有了整体认识，熟悉了各个模块的设计原理和各种实现算法的改进过程。尤其是磁盘管理部分讲解的十分透彻（从生磁盘到扇区，从扇区到磁盘块，最后再引入文件系统），自己对文件系统的理解产生了质的飞跃。

## 0x02 摘录

计算机的发展经历了批处理系统（一个作业完成自动读入下一个作业）到多道程序（作业之间的切换和调度）再到分时系统（在所有用户启动的作业之间快速切换，和多道程序的区别在于并不是执行不下去再切换，而是让每个用户觉得自己的程序一直在执行）的过程。

操作系统是计算机硬件和应用程序之间的一层软件，方便我们高效地使用硬件。操作系统需要管理哪些硬件呢？

- 必须的模块：CPU管理，内存管理，设备管理，磁盘管理（文件系统），还有系统启动和系统接口。
- 扩展模块：网络管理，电源管理，多核管理。

这部分属于高级操作系统内容，国外课程在讲，目前国内讲的很少。

斯坦福是怎么学操作系统呢？当时国内还在学高斯消元法（1800年提出）解方程时，国外就已经用SVD奇异值分解（近几十年提出）了。我们的学生要跟得上潮流。

给你一块板子，要能够从硬件出发，设计出一个操作系统。动手实践一个真实的操作系统。

### 第1部分：操作系统启动3讲

#### 1. 上电启动到0x7c00

计算机的工作原理：核心就是冯诺依曼存储程序思想，取指执行。

打开电源，计算机执行的第一句指令是什么？

刚开机时，x86体系的CPU处于实模式，也就是16位模式。在实模式，只能进行实模式最多20位寻址，使用16位的寄存器。实模式寻址：CS左移4位+IP。

开机那一刻，CS和IP寄存器就被设置了初始值：CS=0xFFFF；IP=0x0000（CS）。因此CPU会到0xFFFF0处取指执行。0xFFFF0对应的就是ROM BIOS代码的映射区。

BIOS负责检查内存、磁盘、显示器等硬件设备，然后将磁盘0道0面1扇区的512字节读入到物理内存的0x7c00处。读入前会检测512字节的最后两个字节是否是0xAA55的主引导扇区标志。

拷贝后设置CS=0x07C0，IP=0x0000。因此接下来CPU就要到跳转到物理地址的0x7C00处执行这512字节的主引导扇区代码。

#### 2. bootsect.s

物理内存的0x7C00存放的就是主引导扇区的代码。接下来的任务就是，利用这512字节的代码最终实现后续整个操作系统的加载，然后跳转到操作系统的第一行代码去执行。

由于512字节太小，无法直接完成对整个操作系统的加载，因此需要采用接力的方式，先由512字节的bootsect代码读取后面四个扇区的setup代码，然后由setup加载全部操作系统代码，最终将控制权转到操作系统第一行代码。

由于操作系统需要在保护模式（32位地址）下运行并且采用分段和分页机制寻址，因此setup在加载完操作系统代码后，需要先初始化GDT表，然后切换到保护模式（打开A20地址线），使得保护模式下能够利用保护模式下的寻址方式（通过CS寄存器查GDT表找到物理内存地址）找到操作系统的第一行代码。

中间还要包括：boot代码要从0x7c00先移动到0x90000处才开始读取4个扇区的bootsetup，然后bootsetup会把操作系统读取到物理地址的0地址处，初始化GDT和LDT表，开始保护模式，最后通过GDT表跳转到0地址执行。

磁盘中的代码分为boot+bootsetup+system os三部分。

0x7c00处存放的代码：

```
BOOTSEG  = 0x07c0
INITSEG  = 0x9000
SETUPSEG = 0x9020

entry start // 关键字entry告诉连接器程序入口

start:
    mov ax, #BOOTSEG  // 段寄存器之间不能直接作数据传送
    mov ds, ax
    mov ax, #INITSEG
    mov es, ax
    mov cx, #256
    sub si, si
    sub di, di
    rep movw  // 将DS：SI的内容送至ES：DI rep重复次数cx
    jmpi go, INITSEG  // 段间跳转 将INITSEG赋值给CS，go赋值给IP

go:
    mov ax, cs  // cs=0x9000
    mov ds, ax
    mov es, ax
    mov ss, ax
    mov sp, #0xFF00

load_setup:
    // 从第2个扇区读4个扇区
    mov dx, #0x0000
    mov cx, #0x0002
    mov bx, #0x0200
    mov ax, #0x0200+SETUPLEN
    int 0x13 // BIOS读磁盘扇区中断 
             // ah=0x02读磁盘 al=扇区数量SETUP=4  
             // ch=柱面号 cl=开始扇区 dh=磁头号 dl=驱动器号 
             // es:bx=内存地址 
    jnc ok_load_setup
    mov dx, #0x0000
    mov ax, #0x0000
    int 0x13


ok_load_setup:
    mov dl, #0x00
    mov ax, #0x0800  // ah=8 获得磁盘参数
    int 0x13

    mov ch, #0x00 
    mov sectors, cx
    mov ah, #0x03
    xor bh, bh
    int 0x10  //读光标位置
    mov cx, #24
    mov bx, #0x0007 // 显示属性
    mov bp, #msg // msg:loading...
    mov ax, #1301
    int 0x10   //显示字符

    mov ax, #SYSSEG  // SYSSEG=0x1000
    mov es, ax
    call read_it   // 读system os内容
    jmpi 0, SETUPSEG
    
read_it:
    mov ax, es
    cmp ax, #ENDSEG
    jb ok_read
    ret

ok_read:
    mov ax, sectors
    sub ax, sread  // sread是当前磁道已读扇区数，ax是未读扇区数
    call read_track  // 读磁道

.org 510
.word 0xAA55    
```

#### 3. setup.s

bootsect应该做什么事？刚上电时，操作系统在磁盘上，而计算机从内存中取指执行，因此必须把代码放到内存中。

bootsect的作用就是分段将代码读进来，首先读的是setup，然后打出loading的logo，然后把system的代码读进来。之后所有的操作系统代码都在内存中了。

之后是执行setup：setup模块，也是一段汇编setup.s。根据名字就能想到：setup完成OS启动前的设置。

int 0x15获得物理内存的大小。

```
start:
    mov ax, #INITSEG
    mov ds, ax
    mov ah, #0x03
    xor bh, bh
    int 0x10    // 取光标位置dx
    mov [0], dx
    mov ah, #0x88
    int 0x15
    mov [2], ax   //扩展内存大小
    ...
    cli  // 不允许中断
    mov ax, #0x0000
    cld
do_move:
    mov es, ax
    add ax, #0x1000
    cmp ax, #0x9000
    jz end_move

    mov ds, ax
    sub di,di
    sub si,si
    mov cx, #0x8000
    rep movsw // 将system模块迁移到0地址
    jmp do_move 
```

取出光标位置到0x90000，取扩展内存大小到0x90002处。

| 内存地址    | 长度  | 名称     |
| ------- | --- | ------ |
| 0x90000 | 2   | 光标位置   |
| 0x90002 | 2   | 扩展内存大小 |
| 0x9000C | 2   | 显卡参数   |
| 0x901FC | 2   | 根设备号   |

setup取出来很多硬件参数，放到0x90000处，然后将0x10000处的操作系统移动到0地址处。

#### 4. 保护模式的寻址方式

cs和ip寄存器都是16位，怎么寻址20位呢？cs左移4位+ip，但也只能寻址2^20次方也就是1M的内存空间，寻址能力是非常弱的。16位机不能满足现代操作系统的要求。因此这种寻址方式需要改变。

16位机切换为32位模式，也叫做保护模式。

16位模式和32位模式的本质区别，也就是cs和IP寻址解释方式不同了。

CR0寄存器最后以为PE位置为1，开启保护模式。第32位PG位置为1，启动分页。

```
mov ax, #0x0001
mov cr0, ax
jmpi 0, 8
```

PE置为1后，CPU的CS和IP寻址就会开启一个新的寻址硬件电路。

即GDT表。

实模式是CS左移4位+IP，保护模式下根据cs查表+ip。

cs这时候就被称之为选择子。

要想硬件电路能选择，GDT表中必须有内容，所有setup最后一段功能就是初始化GDT表。

仿照GDT，通过IDT保护模式下的中断处理函数入口。通过int n在IDT中进行查表。

因此后面也要初始化IDT表。

接下来跳到system模块的代码。

system模块中的第一部分代码是什么？

操作系统有很多源码，必须按照boot+setup+system代码的顺序排列。

通常把操作系统最后做出来的样子就是Image就是boot+setup+system的顺序格式。

#### 5. head.s

一段在保护模式下运行的第一段代码。setup是进入保护模式，head是进入保护模式的初始化。

先设置栈，然后call setup_idt   call setup_gdt又设置了一遍IDT和GDT表。

之前的GDT表是临时建立的，为了跳转到0地址用的。现在的GDT是真正的开始工作的表。

开启20号地址线，就可以访问4G内存了。等等等等。

之前的都是ax，bx是16位模式，head之后的就变为eax和ebx等32位汇编。不开保护模式是不能使用eax的，只能用ax的16位模式。

as86汇编：能产生16位代码的Intel8086汇编。

GNU as汇编：产生32位代码，使用AT&T语法。

还有C语言的内嵌汇编。

```
__asm__("汇编语句"
:输出
:输入
：破坏部分描述)；
```

head.s完成后就执行main.c文件。

```
after_page_tables:
    push $0
    pushl $0
    pushl $0
    pushl $L6
    pushl $_main
    jmp set_paging
L6:
    jmp L6
setup_paging:
    设置页表
    ret
```

setup_paging返回会执行main函数，main函数返回会执行L6死循环。操作系统是个永不停止的main函数。

#### 6. main函数

```
void main(void)
{
    mem_init();
    trap_init();
    blk_dev_init();
    chr_dev_init();
    tty_init();
    time_init();
    sched_init();
    buffer_init();
    hd_init();
    floppy_init();
    sti();
    move_to_user_mode();
    if(!fork()){init();}
}
```

main的工作就是xx_init：内存、中断、设备、时钟、CPU等内容的初始化。

#### 7. mem_init

看一看mem_init 在linux/mm/memory.c中。

就是初始化了一个成为mem_map的数组。

内存的初始化。

mem_map[i++] = 0

4K做一片，变成一页一页的初始化。

void mem_init(long start_mem, long end_mem)

end_mem就是物理内存的大小。

boot setup   head main  mem_init

boot将操作系统读进来。

setup获得启动参数，保护模式

head初始化GDT表和页表。

main一堆init，最后fork执行init进程。

第一个操作系统读到内存，第二步完成初始化。

### 第2部分：操作系统接口2讲

从接口进来来看操作系统到底发生了什么事，来看操作系统的内部原理。

#### 1. 接口是什么？

接口的背后是怎么运行的？

用户如何使用计算机？

命令行、图形按钮、应用程序。

1. 命令行

命令：本质上就是一段程序。

shell本质也是一段程序，扫描输入参数，然后调用fork和exec执行命令。

2. 图形按钮

怎么回事？

图形按钮是基于一套消息机制。

大家完全可以在Linux0.11上实现图形接口。实现鼠标的点击。消息机制。再实现一个绘图。

所谓的图形化界面并不复杂，只是实现一些东西：消息队列：当鼠标点下去时，通过中断放到系统内部的消息队列，应用程序写一个系统调用GetMessage，从内核里把消息取出来，根据拿出来的是什么消息，比如鼠标按下去了，然后执行相应的函数。应用程序是一个不断从消息队列取消息的函数。这就是消息机制。

图形按钮。

无论是命令行还是图形界面，都是一个程序。

普通的C语言，加上一些重要的函数。

命令行：命令程序。

图形界面：消息框架程序+消息处理程序。

操作系统提供这些重要的函数。接口表现位函数调用，又由系统提供，所以称为系统调用。syetem call。

有哪些系统接口呢？

POSIX IEEE制定的一个标准族。

#### 2. 系统调用的实现

背后的原理。

实现一个whoami系统调用。

用户程序调用whoami，一个字符串存放在操作系统中（系统引导时载入）。取出来打印。应用程序不能随意调用内存，不能随意jmp。

不可以jmp

凭什么不让应用程序jmp

将内核程序和应用程序隔离。

内核态，用户态。内核段，用户段。

区分内核态和用户态：一个处理器硬件设计。

当前程序执行在什么态？由于CS：IP是当前指令，所以用CS的最低两位表示：0是内核态，3是用户态。

内核态可以访问任何数据，用户态不能访问内核数据。

对于指令跳转也一样实现了隔离。

CS寄存器最后两位CPL，当前特权级。

目的寄存器最后两位RPL

RPL >= CPL，当前特权级是3

DPL

cs在gdt表中，gdt表记录了权限等级，执行指令必访存，访存必查表，gdt表必出现权限等级位，等级位必被检查，一气呵成。

硬件也提供了主动进入内核的方法。

对于intel x86，那就是中断指令int。

int指令是将CS中的CPL改成0，进入内核。这是用户程序发起的调用内核代码的唯一方式。

系统调用的核心：

- 用户程序包含一段包含int指令中断的代码。
- 操作系统写中断处理：获取想调程序的编号。
- 操作系统根据编号执行相应的代码。

int 0x80中断指令，系统调用。DOS是21H。

Linux系统调用的实现细节：

int 0x80到底做了什么事。

#define __NR_write 4，放在eax中，ebx，ecx，edx存放3个参数。

将系统调用号给eax，然后调用int 0x80触发中断进入操作系统。

eax标记到底因为什么进入到内核。

查IDT表看0x80跳转到哪里。

```
void sched_init(void)
{
    set_system_gate(0x80, &system_call); // 初始化IDT 0
}
```

从int 0x80执行相应的中断处理程序，然后回来。

中断处理程序system_call

call sys_call_table(%eax, 4)-->sys_write

调用printf，库函数printf，库函数write，系统调用write（CPL=3），int 0x80（DPL也等于3）进入（CPL被置为0）内核中断处理system_call，查表call sys_call_table，到sys_write。

### 第3部分：CPU管理12讲

#### 1. CPU管理的直观想法

管理CPU的时候，引出了多进程图像。

CPU管理明白了，其它硬件都带着管理好了。

管理CPU，首先要使用CPU。

只有CPU用起来了，才能谈得上管理。

#### 1. CPU的工作原理

CPU上电以后发生了什么？

PC=50，取指执行。将50处的指令读入CPU解释执行。

CPU怎么工作？自动的取指执行。给它一个地址，从内存把指令取出来执行。再给一个再取出来执行。一旦给了第一个地址，PC是自动累加。只要给一个初始地址，CPU就不断地取指执行。CPU就开始工作了。

CPU才能让它工作起来呢》给它一个初始地址即可。设好了PC的初值就完事了。

管理CPU最直观的想法，设一个PC初值为一段程序的开始地址。

#### 2. 多道程序

有没有什么问题？提出问题。

IO指令和计算指令耗时不一样。访问IO的时间远大于CPU计算。

IO操作比较慢，IO期间必须等待，CPU利用效率低了。

怎么解决？

多道程序。

多个程序在内存中来回切换交替执行，让CPU忙碌起来。

多道程序，交替执行。就是CPU管理的核心。并发。

#### 3. 进程

如何实现进程之间的切换？修改寄存器PC就行了吗？

要记录需要返回的PC地址，还要记录寄存器。需要记录切换出去时程序执行到哪里（PC值），执行的当前状态（寄存器）。每个程序有了一个存放信息的结构：PCB。

运行的程序和静态程序不一样，需要描述运行期间的样子，这就是进程概念。程序和进程的本质区别。

进程是运行中的程序。

CPU的

启动一个进程，让CPU去执行这个进程。

CPU管理：让CPU启动多个进程，跑多个进程。

### L9 多进程图像

#### 1. 什么是多进程图像

xx

如何使用CPU呢？让程序执行起来。

如何充分利用CPU呢？启动多个程序，交替执行。

启动了的程序就是进程，所以是多个进程推进。操作系统只需要把这些进程记录好、要按照合理的次序推进（分配资源、进行调度），这就是多进程图像。

多进程使用CPU的图像

多进程图像从系统启动开始到关机结束。

main中的fork创建了第1个进程。

init执行了shell（Windows桌面）。

shell再启动其它进程。

#### 2. 多进程如何组织

xx

PCB：记录进程信息的数据结构。

用PCB组成就绪队列。

有一个进程正在执行，有一些进程等待执行，有一些进程再等待某个事件。

多个进程所对应的PCB分别放在不同的地方，有就绪队列，有等待IO的队列。

多进程的组织：PCB+状态+队列。

把进程通过状态来区分开来。

新建态，就绪态，运行态，终止态，阻塞态。

进程状态图。

操作系统让进程在不同状态之间相互转化。

#### 3. 多进程如何交替

多进程图像必须实现切换。

一个进程启动磁盘读写，就必须等待，要等待就要切换。操作系统把该进程的状态变为阻塞态，然后放到DiskWaitQueue队列中，最后调用schedule切换函数。

```
    schedule()
{
    pNew = getNext(ReadyQueue);
    switch_to(pCur, pNew);
}

```

交替的三个部分：队列操作+调度+切换。这就是进程调度。

如何调度呢？

FIFO，显然是公平的策略。但显然没有考虑进程执行的任务的区别。

Priority：优先级怎么设定？可能会使某些进程饥饿。

switch_to(pCur, pNew)首先pCur.eax=CPU.eax;CPU.eax=pNew.eax保存当前进程的寄存器，将新调入的进程寄存器赋值给CPU。

#### 4. 多进程如何影响

多个进程同时存在于内存中就会出现问题：两个进程访问同一内存。

解决办法：限制对同一地址的读写。多进程的地址空间分离：映射表，内存管理。

通过映射表实现地址空间的分离。

两个进程同时访问100，进程1通过映射表100映射为780，进程2映射为1260，互不冲突。

#### 5. 多进程如何合作

打印进程。

打印队列：有的进程往打印队列中放内容，有的进程从打印队列取内容。要保证两个进程同时放时不乱。这就是合作。

生产者消费者实例。共享缓冲区。

生产者放内容，消费者取内容。

如果缓冲区满了，就不应该再放。两个合作的进程都要修改counter。

核心在于进程同步。合理的推进顺序。

总结：

如何形成多进程图像？

读写PCB，OS中最重要的数据结构，贯穿始终。

要支持多进程图像，操作系统需要实现：

操作寄存器完成切换（L10，L11，L12）。

要写调度程序（L13，L14，L15）。

要有进程同步与合作（L16，L17）

要有地址映射（L20-L25）。

### L10 用户级线程

操作系统做什么事，可以让进程切换起来。

多进程时操作系统的基本图像。

进程是一堆指令，指令访问的内存通过映射表获取。

一个指令序列切换到另一个指令序列。

从一个A函数切换到B函数去执行。只切换指令，不切换映射表可不可以？

是否可以资源不动而切换指令序列？

进程=资源+指令执行序列。

将资源和指令执行分开，一个资源+多个指令执行序列。

也就是多个进程使用同一个内存映射表。既能保证多个指令交替执行，保留了并发的优点，又避免了切换造成的代价（不用切换映射表等资源了）。

本次切换讲指令的切换，也就是寄存器的切换。

进程的切换包含两个部分：一个是指令的切换，一个是内存映射表等资源的切换。

两个部分分开，分而治之。

先讲指令的切换，再讲资源的切换。

多个执行序列共用一个地址空间是否实用？

一个网页浏览器，一个线程用来从服务器接收数据，一个线程用来显示文本，一个线程用来处理图片，一个线程用来显示图片。这些线程需要共享资源吗？

接收数据放在100处，显示都需要读。所有的文本、图片都显示在一个屏幕上。

多线程的切换只是比多进程的切换少了资源切换的问题。

先通过多线程的切换讲解指令切换。

多线程的切换，核心是Yield函数。

能切换了就知道切换时需要是个什么样子。create就是要制造出第一次切换时应该的样子。

仔细看Yield。

线程函数调用时会把Yield压入，先执行Yield进行切换。

两个执行需要共用一个栈，会出问题。

因此，两个线程得两个栈。

函数调用时指令序列内部的事情。

也就是线程TCB结构。

因此Yield要先切换栈。

```
void Yield()
{
    TCB2.esp=esp;
    esp=TCB1.esp;
}
```

两个线程的样子：两个TCB、两个栈、切换的PC在栈中。

ThreadCreate函数的核心就是用程序做出来这三样东西。

```
void ThreadCreate(A)
{
    TCB *tcb = malloc();
    *stack = malloc();
    *stack = A;
    tcb.esp = stack;
}
```

将所有的东西组合在一起。

为什么先讲用户级线程？用户级线程也可以实现这种切换，而且这种切换完全是由用户主动做的，这样讲起来比较容易，不用进内核。

用户级线程切换是内核线程切换的一个子集。

为什么说是用户级线程——Yield是用户程序。

如果进程的某个线程进入内核并阻塞，那么如果一个线程通过系统调用进入到内核呢？内核就要切换到别的进程，也就是在内核中需要进程Schedule。

用户层是线程Yield。

内核层是进程Schedule。

核心级线程：

ThreadCreate是系统调用，会进入内核，TCB在内核中。

内核级线程的并发性比用户级线程会好一些。

一个栈到两个栈，支持线程的切换。

Yield变为用户不可见，调度点由系统决定。

### L11 内核级线程

#### 1. 内核级线程的由来

切换指令流的本质就是线程切换。

切换进程本质上是切换内核级线程，而不是用户级线程。切换用户级线程对切换内核级线程的理解是非常重要的。

如何切换内核级线程？

系统中往往用户级线程也有，内核级线程也有。

一个系统如果不支持核心级线程，也就不能使用多核。

MMU可以理解为内存映射表，多处理器的每个CPU都对应一个MMU，多核的所有CPU只对应一个MMU。

并发：

并行：同时触发，交替执行。

用户级线程和多进程都不能发挥多核的优势，只有内核级线程才能发挥多核的优点。

进程之间的切换会改变映射表的内容，花费时间多，线程不改变正好使用了多核的单个MMU。

和用户级相比，核心级线程有什么不同?

能否实现只有核心级线程的操作系统？

ThreadCreate是系统调用，内核管理TCB，内核负责切换线程。

如何让切换成型？内核栈，TCB。

用户栈是否还要用？执行的代码仍然在用户态，还要进行函数调用。

一个栈到一套栈；两个栈到两套栈。

用户层，用户栈；内核层，内核栈。

要有核心级线程，就必须既要有用户栈，又要有内核栈。

TCB关联内核栈，那用户栈怎么办？

用户级线程用的是两个用户栈，而内核级线程用的是两套栈。

每个内核级线程都要有用户栈和内核栈两套栈。

核心级线程切换时，TCB切换一套栈。内核栈和用户栈同时切换。

#### 2. 用户栈和内核栈之间的关联

进入内核的唯一方法，就是中断。

只要有int指令，就启用内核栈。那怎么找到内核栈呢？

每个线程都对应一个用户栈和一个内核栈。

一旦int指令，操作系统计算机硬件通过硬件的寄存器找到每个线程对应的内核栈，找到内核栈后，还要往内核栈压一些东西，压的用户栈的SS和SP，CS和PC，也就是用户层的执行到哪里，用户栈等状态。这就是一套栈。

int指令和iret指令。实现了一套栈的思想。SS、SP\EFLAGS、PC、CS，

A调用read系统调用，认真体会从内核返回时的样子。

内核启动磁盘读时，将自己变成阻塞，将自己设置成阻塞状态后就开始调度，找到next，switch_to(cur, next);找到下一个线程。

switch_to仍然是通过TCB找到内核栈指针，然后通过ret切到某个内核程序，最后再iret指令用CS:PC切换到用户程序。

最关键的地方来了，线程T创建时如何填写？

内核线程切换的switch_to五段论：

1.中断入口：进入切换

2.中断处理：引发切换。启动磁盘读写或时钟中断时调用schedule切换。

3.schedule找到next的TCB，然后调用swtich_to完成内核栈切换。

4.switch_to完成内核栈切换

5.中断出口：切换到用户层（第二级切换）。iret指令。

如果非同一进程的两个线程如何切换？首先要切换地址映射表，也就是内存管理。

如何实现内核级线程的ThreadCreate函数呢？

```
void ThreadCreate()
{
    TCB tcb = get_free_page();
    *krlstack = ...;
    *userstack传入;
    tcb.esp = krlstack;
    tcb.status = ready;
    tcb入队列
}
```

用户级线程和内核级线程的对比。

linux0.11不支持内核级线程，但和进程切换非常相似。

用户级线程和内核级线程结合。

### L12 内核级线程的实现

xx

内核级线程的原理就是TCB引发的栈的切换。

进程由两部分组成：一部分是资源，一部分是执行序列。执行序列就是线程，内核级线程。资源管理主要是内存管理。操作系统必须支持进程。可以不支持内核级线程和用户级线程。

核心级线程的两套栈，核心是内核栈。

从用户栈到了内核栈，从内核栈找到TCB完成切换，内核栈也就完成了切换，再用iret切换到用户栈。

整个故事要从进入内核开始：某个中断开始。

fork：fork是系统调用，会引起中断。

fork创建进程，创建资源和指令序列，也就是创建线程。

```
__NR_fork
int 0x80
```

中断入口：

初始化时将各种中断处理设置好。

判断进程状态和时间片，状态阻塞或时间片用完了就重新切换reschedule。

中断出口。

switch_to:

TSS段

linux0.11用的是TSS段切换，指令简单，但效率低。

要改为kernel stack切换。

ljmp长跳转把当前所有寄存器放在当前TR寄存器指向的TSS段。用下一个TSS段拷贝至所有的寄存器上。

Linux0.11用tss段进行切换，但也可以用栈切换，因为tss中的信息可以写到内核栈中。

核心也就三条指令：int  switch_to的ljmp，最后一个iret。

用栈的方式实现整个内核级线程，代码也就100句以内。

另一个故事ThreadCreate也就顺利了：

从sys_fork开始CreateThread。

sys_fork调用copy_process，细节：创建栈。

申请内存空间，创建TCB，创建内核栈和用户栈，填写两个stack，关联栈和TCB。

第三个故事：如何执行我们想要的代码

子进程用了父进程的壳子。fork何时返回0，何时不会。首先要找到fork怎么返回。

```
mov %eax,
```

结构：子进程进入A，父进程等待。

故事要从exec这个系统调用开始。

理解switch_to对应的栈切换，将自己变成计算机。ThreadCreate的目的就是初始化这样一套栈。

### L13 操作系统的那棵树

复杂系统将会成为核心。

人的头脑不是盛水的容器，而是一团需要点燃的火。

一棵树不是从上大小垒石头垒出来的，而是从小树长成了大树。也就是现有主干，再慢慢扩充。

思考。

运转CPU：

怎么使用CPU呢？PC设置初始值。紧接着由于IO阻塞导致CPU没有好好运转。得让CPU好好运转，阻塞时多道切换。

从A跳到B，用栈来做Push PC  pop PC。

一个栈+Yield造成的混乱。面对这样的栈怎么可能从B顺利回到A。就变成两个栈+两个TCB，Yield找到下一个TCB，找到新的栈，然后切换到新的栈。

一直在用户态那怎么行？

引入内核栈的切换。

用户栈到内核栈，内核栈到TCB。

到实现idea的时候开始了。

从一个简单、清晰、明确的目标开始。在屏幕上交替打印A和B。

linux0.01就是在屏幕上交替打出来A和B。

程序是什么？就是人的思维的C表达。

中断返回的时候调用schedule。

schedule的调度点：什么时候调度呢？中断进入内核后。还可以定时中断或者高优先级抢占。

发散思维。

有那么一次时钟中断。

### L14 CPU调度策略

调度就是接下来调度哪个进程。

如何获取next进程。

#### 1. CPU调度的直观想法

FIFO？

谁先进入先调度谁：简单有效。一个只简单询问业务的人怎么办？

Priority？

任务短可以适当优先，但你怎么知道这个任务将来会执行多长时间呢？这人的询问越来越差怎么办？

面对诸多场景，如何设计调度算法？

我们的算法应该让什么更好？让进程满意。

时间。

尽快结束任务。用户操作尽快响应。系统内耗时间少。

总原则：系统专注于任务执行，又能合理分配任务。

如何做到合理？需要折中，需要综合。

吞吐量和响应时间之间有矛盾。前台任务和后台任务的关注点不同。

IO约束型任务和CPU约束型任务各有各自的特点。

折中综合让操作系统变得复杂，但有效的系统又要求尽量简单。

#### 2. 各种CPU调度算法

FCFS 先来先服务。

如何缩短周转时间？SJF：短作业优先。

响应时间该怎么办？如何解决？按时间片来轮转调度。

时间片大：响应时间太长；时间片小：吞吐量小。

折衷：时间片10-100ms，切换时间0.1-1ms。

响应时间和周转时间同时存在，怎么办？

一个调度算法让多种类型的任务同时满意，怎么办？

直观想法：定义前台任务和后台任务两个队列，前台RR，后台SJF，只有前台任务没有时才调度后台任务。

优先级调度。

如果一直有前台任务。

怎么办？后台任务优先级动态升高，但后台任务一旦执行，前台响应的时间。

前后台任务都用时间片，但又退化为了RR，后台任务的SJF如何体现，前台任务如何照顾？

还有很多问题，哪些是前台任务，哪些时后台任务，fork时告诉我们吗？

gcc就不需要交互吗？

SJF中的短作业优先级如何体现？如何判断作业的长度？这是未来的信息。

### L15 一个实际的schedule函数

Linux 0.11中循环遍历PCB数组，找到状态时TASK_RUNNING且counter最大的PCB，counter既做时间片又做优先级。

#### 1. counter做时间片

counter的作用：时间片：在时钟中断时，当前进程的counter自减，等于0时就执行shedule函数。

counter是典型的时间片，所以是轮转调度，保证了响应速度。

#### 2. counter做优先级

counter的另一个作用：优先级。shedule中找出最大的任务调度，counter表示了优先级。counter代表的优先级也能够在shedule中动态调整，保证IO返回时的进程优先级高（counter小）。阻塞的进程再就绪以后优先级高于非阻塞进程。

阻塞的进程越久，它的counter增加的越大，优先级越高。

counter的作用整理

counter保证了响应时间。

经过IO以后，counter就会变大；IO时间越长，counter越大，照顾了IO进程，变相的照顾了前台进程。

后台进程一直按照counter轮转，近似了SJF调度。

每个进程只维护一个counter变量，简单高效。

CPU调度：一个简单的算法折衷了大多数任务的需求，这就是实际工作的schedule函数。

### L16 进程同步与信号量

让多个进程的合作变得合理有序。

进程合作：多进程共同完成同一个任务。

从纸上到实际：生产者—消费者实例。

等待是多进程合作的核心。

需要让进程走走停停来保证多进程合作的合理有序，这就是进程同步。

缓存区满，生产者要停。缓存区空，消费者要停。

只发信号还不能解决全部问题。

无法表达出来记录到底是几个生产者在睡眠。仅仅发一个信号就不行了，而是得用信号量。

#### 1. 从信号到信号量

除了睡眠和唤醒以外，还得记录其它信息。比如到底多少个进程在睡眠。

信号量开始工作。

根据信号量来唤醒。

信号量记录多少个进程在睡眠，维护一个唤醒队列。

什么是信号量？

#### 2. 信号量的定义

信号量：一种特殊整型变量，信号量结构体，整数记录资源个数，指针记录等待在该信号量上的进程。

Linux 0.11没有信号量，增加进去。

用信号量解决生产者消费者的问题。

### L17 信号量临界区保护

#### 1. 为什么要保护信号量？

什么是信号量：通过对这个量的访问和修改，让大家有序推进。

共同修改信号量引出的问题。

竞争条件：和调度有关的共享数据语义错误。

错误由多个进程并发操作共享数据引起。错误和调度顺序有关，难于发现和调试。

#### 2. 用什么方法保护信号量？

解决竞争条件的直观想法：

在写共享变量empty时阻止其它进程也访问empty。

一段代码一次只允许一个进程进入。

#### 3. 临界区

临界区：一次只允许一个进程进入的该进程的那一段代码。

一个非常重要的工作：找出进程中的临界区代码。

临界区代码的保护原则

基本原则：互斥进入。如果一个进程在临界区中执行，则其它进程不允许进入。

这些进程间的约束关系称为互斥。这保证了是临界区。

好的临界区保护原则：

有空让进。有限等待。

#### 4. Peterson算法

进入临界区的一个尝试：轮换法

又一个尝试：借鉴生活中的道理。

标记法。

进入临界区，打个标记。会造成请求无限等待。

再一次尝试：非对称标记。

带名字的便条，让一个人更加勤劳。

进入临界区Peterson算法

结合了轮转和标记两种思想。

满足互斥进入，有空让进，有限等待。

#### 5. 面包店算法

多个进程怎么办？面包店算法。

仍然是标记和轮转的结合。

如何轮转：每个进程都获得一个序号，序号最小的进入。

如何标记：进程离开时序号为0，不为0的序号即标记。

一个显然的感觉是：太复杂了，有没有简单一些的。

#### 6. 关中断

纯用软件来做，太复杂，得用硬件来做。

临界区保护的另一类解法：再想一下临界区，只允许一个进程进入，进入另一个进程意味着什么？

进入另一个进程意味着什么？被调度：另一个进程只有被调度才能执行，才可能进入临界区，如何阻止调度？

要想调度，必须要执行schedule，要调用schedule，必须进入内核，也就是中断。

因此在临界区前关中断，临界区后再开中断。

```
cli
临界区
sti
```

CPU中的中断是CPU之上有个中断寄存器INTR，中断寄存器置位就表示来中断。CPU在每执行一条指令后就检查一下INTR寄存器，如果执行cli后，CPU就不会再检查中断寄存器。

什么时候不好使：但在多核场景下，cli只能控制当前CPU的中断寄存器，无法控制其它CPU的中断寄存器，那怎么办呢？

关中断对多核CPU不好使。

#### 7. 原子锁

另一种方法：原子指令

临界区保护的硬件原子指令法。

硬件锁：原子指令。也就是硬件实现的整数为1的信号量。

原子锁因为是多内存访问的锁，因此也适合多核CPU的场景。

用临界区去保护信号量，用信号量实现同步。

### L18 信号量的代码实现

信号量是原理比较简单，实现比较简单。

进程同步在很多场景中使用，比如多个进程对磁盘的访问。

写应用层程序调用信号量的系统调用实现进程同步，没有信号量接口的操作系统，去实现。Linux0.11没有信号量系统接口，需要做实验实现。

用户层sem_open，sem_wait中调用schedule。

从linux0.11那里学点东西：

读磁盘块过程中的同步，启动磁盘读以后睡眠，等待磁盘读完由磁盘中断将其唤醒，也是一种同步。read系统调用中也使用了cli和sti关中断来实现信号量。

while(lock)的理解。

### L19 死锁处理

再看生产者消费者的信号量解法，这个反复琢磨是无穷无尽的。

多个进程由于互相等待对方持有的资源而造成的谁都无法执行的情况叫做死锁。

#### 1. 死锁的成因

资源互斥使用，一旦占有别人无法使用。

进程占有了一些资源，又不释放，再去申请其它资源。

各自占有的资源和互相申请的资源形成了环路等待。

死锁的4个必要条件

互斥使用，不可抢占，请求和保持，循环等待。

#### 2. 死锁处理方法概述

死锁预防：破坏死锁出现的条件。

在进程执行前，一次性申请所有需要的资源，不会占有资源再去申请其它资源。

缺点：需要预知未来，编程困难；许多资源分配后很长时间才会使用，资源利用率低。

对资源类型进行排序，资源申请必须按序进行，不会出现环路等待。

死锁避免：检测每个资源请求，如果造成死锁就拒绝。

如果系统中的所有进程存在一个可完成的执行序列P1P2P3...Pn，则称系统处于安全状态。

怎么样才不死锁呢？

死锁避免：判断此次请求是否引起死锁？判断是否不是当前状态，不死锁。

死锁检测和恢复：检测到死锁出现时，让一些进程回滚，让出资源。

死锁忽略：就好像没有出现死锁一样。

安全序列如何找？

#### 3. 银行家算法

找出安全序列的银行家算法：Dijkstra提出。

每种资源剩余数量，已分配资源数量，进程还需要的各种资源数量，然后算出下一次该分配给哪个进程。

时间复杂度太高。代价非常大。

死锁避免之银行家算法。

请求出现时，首先假装分配，然后调用银行家算法。如果不行，则此次申请被拒绝。

死锁检测和恢复：发现问题再处理。

基本原因：每次申请都检测是否死锁会导致时间复杂度高，效率低。那就等发现问题再处理。

定时检测或者是发现资源利用率低时进行检测。

deadlock进程组。死锁被阻塞进程组。

选择进程进行回滚。选择哪些进程回滚？优先级？占用资源多的？哪个都不好。

因此许多通用操作系统，比如Windows和Linux都采用死锁忽略的方法。死锁忽略的代价最小。

## 第4部分：内存管理6讲

### L20 内存使用与分段

#### 1. 如何让内存用起来？

仍然从计算机如何工作开始。取指执行。取指就是从内存中取。

首先把程序放到内存。base+offset。程序加载的基地址。base。

call 40

重定位：修改程序中的地址（相对地址，也叫逻辑地址）。offset。

什么时候完成重定位？编译时（烧进去后不再变化，卫星等嵌入式程序）或载入时。

编译时重定位的程序只能放在内存固定位置。

载入时重定位的程序一旦载入到内存就不能动了。

程序载入后还需要移动：

swap交换。

内存中的进程1长时间阻塞睡眠，将进程1换出到磁盘。

在运行每条指令时才完成重定位。每执行一条指令都要从逻辑地址算出物理地址：地址翻译。

重定位最合适的时机：运行时重定位。

PCB-->base字段。执行指令时第一步先从PCB中取出这个基地址。

总结：

在内存中找一段空闲内存，然后把这个地址赋值给PCB.base。程序载入到这段内存，PC设置好初始地址，开始执行。

#### 2. 引入分段

是将整个程序一起载入到内存中吗？程序员眼中的程序：由若干段组成，每个段有各自的特定、用途。程序段cs、数据段ds、栈。代码段只读，代码/数据段不会动态增长。段号，段内偏移。将各段分别放入内存。

进程段表：每个进程都要一个段表。LDT表。

GDT表：操作系统OS这个大进程对应的段表。

GDT+ LDT。

总结：

每个段放到一个空闲内存，把基址放到LDT表中，LDT表放到PCB中。

### L21 内存分区与分页

主题：操作系统管理内存。

冯诺依曼结构的本质：把一段程序放到内存中，然后由CPU取指执行。

按照分段放到内存，然后按照运行时重定位取指。LDT表。ldtr寄存器。LDT表放在PCB。

三个事情：将程序分成多个段（编译器负责），要在内存中找一块空闲分区（一个算法），磁盘读写把程序载入到这块空闲的分区（驱动IO），读的时候把LDT表初始化。

#### 1. 内存分区

内存怎么分割？

固定分区与可变分区。

将内存等分成k个分区。但段有大有小，需求不一定。

段请求：可变分区。

可变分区的管理：请求分配。

已分配分区表，空闲分区表。

再次申请：空闲分区中多个大小都满足，选哪个呢？

操作系统的很多算法没有对错，只有优劣之分。根据具体情况来综合衡量。

最佳适配：选一个大小最接近的。

最差适配：选一个最大的。

首先适配：分区表第一个。

操作系统负责找到哪些分区是空闲的，维护空闲分区表和已分配分区表，并提供一个算法提供内存请求。

#### 2. 分页

引入分页：解决内存分区导致的内存效率问题。

可变分区造成的问题：总空闲空间>160，但没有一个空闲分区>160，导致无法使用，这就是内存碎片问题。

内存紧缩：将空闲分区合并，需要移动。花费大量时间，不可行。

从连续到离散：将面包切成片，将内存分成页。针对每个段内存请求，系统一页一页的分配给这个段。不需要内存紧缩，最大的内存浪费是4K。

物理内存关心的问题：没有浪费。

用户关系的问题：分段。

页已经载入了内存，接下来的事情：

jmp 40

要计算出来40放到哪一页了。这时候就要用到页表。

段表的寄存器ldtr，页表的寄存器是cr3。

0-11共12位4K，12-15位表示Page第几页。

程序由多个段组成，一个段要分成好多页，这些页打散放在物理内存的页框里。为了实现重定位，根据地址除以4K，得到第几页，然后从页表找到页对应的页框，最后算出物理地址。

问题：在分页中如何找到是哪个段，然后再找到段中的第几页呢？一个段有一个页表？

### L22 多级页表与快表

解决分页的问题：为了提高内存空间利用率，页应该小，但页小了页表就大了。

程序内可访问地址2^32=4G/4K=1M数量的逻辑页号，2^20个页面，页表就非常大。每个页表4字节，需要4M内存。系统中有100个进程，就需要400M内存。

这很显然是内存的浪费。

实际上大部分逻辑地址根本不会用到。32位总空间4G。

因此看起来一个进程4M的页表并不需要这么多。

#### 1. 第一种尝试：只存放用到的页。

用到的逻辑页才有页表项。但页表的页号不连续，就需要比较、查找。折半查找，也挺慢。因此页表必须要连续。根据下标查找时间O1。

大页表占用内存，造成浪费。

既要连续又要让页表占用内存少，怎么办？多级页表。

#### 2. 第二种尝试：多级页表，即页目录表+页表。

10bit页目录号+10bit页号+12bit Offset。

2^10个目录项*4字节地址=4K，总共需要16K。

多级页表既保证了逻辑页表表项的连续性，查找速度快，又保证了内存中存放的页表要少。

多级页表提高了空间效率，但在时间上增加了访问内存的次数，尤其是64位系统。

#### 3. 快表

TLB是一组相联快速存储，是寄存器。

存放最近使用的页号对应的页表。根据页号直接找到物理页框。TLB未命中时再从多级页表中查找。

块表+多级页表组合而成的结构：多级页表提高了空间利用率，TLB提高了速度。

TLB越大越好，但TLB很贵，一般都是64到1024之间。

相比2^20个页，64很小，为什么TLB就能起作用？空间局部性。程序多体现为循环、顺序结构。

### L23 段页结合的实际内存管理

段页结合：程序员希望使用段，物理内存希望使用页。

#### 1. 如何结合

逻辑地址--线性地址--物理地址。

段页同时存在：段面向用户，页面向硬件。

段页同时存在是为了重定位。

段号+偏移，先通过段表找到段的基址，计算出虚拟地址，然后通过页表找到物理地址。

段页同时存在。

#### 2. 如何具体实现

一个实际的段页式内存管理。

内存管理核心就是内存分配，所以从程序放入内存、使用内存开始。

分配段、建段表；分配页，建页表。

进程带动内存使用的图谱。从进程fork中的内存分配开始。

段页式内存下程序是如何载入内存？

分配虚存，建段表。每个进程的代码段、数据段都是一个段。每个进程占64M虚拟地址空间，互不重叠。

分配内存，建页表。

早期多个进程既可以共用一个虚拟内存，现在每个进程都单独有一个虚拟内存。

只要段表和页表弄好，执行指令时MMU自动计算。

### L24 内存换入

虚拟内存必须要有内存换入换出机制。

内存管理的核心，基于虚拟内存的分段+分页，而用换入换出实现虚拟内存。

swap in

用户进程看到的是0到4G的虚拟内存空间。用户可以随意使用该内存，就好像单独拥有4G内存一样。内存怎么映射到物理内存，用户全然不知。核心要在这个映射过程做文章。

用换入换出机制实现“大内存”的效果。

虚拟内存4G，实际物理内存1G怎么办？访问p=0-1G时，将这一部分映射到物理内存；访问3-4G时，再映射这一部分。

代码数据从磁盘读到物理内存并建立虚拟内存的映射，就叫做内存换入。

请求的时候才换入并建立映射。

请求-->调入页面-->建立页面的映射。

请求调页：

页表中起初没有对应的物理页，程序缺页异常。MMU设备查页表发现缺页，就会触发一个缺页“中断”给CPU。MMU会把寄存器的一个位置位缺页状态。中断处理程序，把程序从磁盘换入到物理内存，然后建立虚拟内存的映射。再重新执行最后一条指令。

磁盘是内存的仓库，换入换出本质上保证了把物理内存当作虚拟大内存使用。时间换空间。

#### 一个实际系统的请求调页

从哪里开始？请求调页，当然从缺页中断开始。

系统初始化时设置中断号，14号Page Fault。

内存管理、进程管理、系统调用、系统初始化。设备驱动，磁盘管理。把这些模块糅合在一起，操作系统就呼之欲出了。

读程序段没问题，如果不用的栈和堆等动态内存呢？也存储到磁盘吗？

**内存换入的本质**：中断处理程序中做三件事：do_no_page：申请空闲页，在磁盘上把这一页读进来，建立页表映射。

### L25 内存换出

swap out

有换入就应该有换出。

page=get_free_page

并不能总是获得新的页，物理内存是有限的。

需要选择一页淘汰，换出到磁盘，选择哪一项？用什么算法找一页来淘汰呢？

FIFO，最容易想到，最先换入的先换出去，但如果刚换入的页马上又要换出了怎么办？

有没有最优的淘汰方法？

最优淘汰算法能不能实现，是否需要近似？LRU算法。

LRU不一定放在get_free_page。其它地方运行LRU算法，与get_free_page配合使用。

算法的改进过程：

#### 1. FIFO页面置换

ABCDADBCBB进程顺序，总共3个物理页。如果D刚把A替换掉后又需要置换A页，这种现象就叫做内存抖动。

因此FIFO并不合适，对算法的评价准则：缺页次数，缺页次数越少越好。尽可能少的触发缺页中断。

#### 2. MIN页面置换

MIN算法：选最远将使用的页淘汰，是最优方案。书上叫OPT算法。

MIN算法需要知道将来哪个程序被执行，但这是不可能的。这个算法带有未来函数。

#### 3. LRU页面置换

用过去的历史预测未来。LRU算法：选**最近最长一段时间没有使用的页**淘汰（最近最少使用）。

面试官：手写一个LRU算法。Least Recently Used。

统计过去Page个请求，找出k个最小的，在这里面取离当前请求最远的。缓冲的概念，有缓冲就会有换入换出问题。

##### LRU的准确实现：时间戳

每个页维护一个时间戳。每个时刻更新哪个页在这一时刻使用的CPU，选具有最小时间戳的那个页置换。

这个算法很容易实现，但真放在操作系统中实际使用，却是非常困难的，不可行的。

每执行一条指令，MMU就要取一次指令，为每个页标记时间戳，很可能溢出，也会很慢。

##### LRU准确实现：页码栈

xx

xx

##### LRU近似实现：将时间计数变为是和否

每次访问一个页，硬件自动设置该位。

选择页淘汰：扫描该位，是1时清零，并继续扫描；是0时淘汰。

二次机会算法：Second Chance Replacement。

SCR这一实现方法称之为Clock Algorithm。

#### 4. Clock算法的分析与改造

如果缺页很少，会出现什么情况？

硬件置位很快R=1，而如果置换程序把R=0这一循环过程很慢，每次置换程序R=0后都被硬件很快置为R=1。置换程序scan一圈后淘汰当前页，也就是第一页。hand指针前移一位。退化为FIFO。效果就不好了。

怎么办？**定时清除R位**。再来一个扫描指针！

用来清除R位，移动速度要快。

用来选择淘汰页，移动速度慢。快慢指针。十分像clock。

置换策略有了，还需要解决一个问题：给进程分配多少页框呢？也就是给每个进程多少物理内存呢？分配的多缺页少。

系统内进程增多，每个进程的缺页率增大。缺页率增大到一定程度，进程总等待调页完成。CPU利用率降低。进程进一步增多，缺页率更大。

这一现象称之为颠簸。一直缺页导致频繁换入换出，也叫做抖动。

分配的页框要能够覆盖到程序执行的一个局部。

工作集概念，决定分配多少个页框。

swap in和swap out结合起来：

磁盘中的swap分区。

swap分区管理：swap in和swap out。

内存换入：从磁盘中读取缺页。

内存换出：淘汰某一页到磁盘来给新的页分配物理内存。

内存管理和进程管理是操作系统的核心，再加上设备驱动和文件系统再假设系统接口，系统初始化系统引导，整个就完成了。

## 第5部分：设备管理2讲

### L26 IO与显示器

#### 1. 外设是如何工作的

计算机是如何让外设工作的？

使用外设，每个外设都有一个驱动卡，比如显卡。

CPU发起指令从PCI总线给显卡中的寄存器，显存，外设对应的控制器。

CPU发出写命令就去干别的事去了

外设完成工作后，向CPU发出一个中断。读数据到内存。

要使用外设，就是CPU向外设控制器发指令。本质out xx,al语句。

在中断处理就可以了。

控制器完成真正的工作，并向CPU发中断信号。

向设备控制器的寄存器写不久可以了吗？

需要查寄存器地址，内容的格式与语义。操作系统要给用户提供一个简单的视图，文件试图，这样方便。

设备驱动本质上做三件事：一是让外设执行的一堆out指令，二是外设工作完以后的中断处理，三是为了让使用外设变得简单操作系统给用户程序提供的统一视图用户接口。

#### 2. 一段操纵外设的程序

不论设备设备都是open，read，write，close。操作系统为用户提供的统一接口。

不同的设备名对应了不同的设备。

一个统一的视图：文件试图。操作系统两大视图之一。

开始给显示器输出。

printf展开部分，最后调用系统调用write。

write中针对不同的设备有不同的处理分支。fd对应PCB中inode的信息。inode描述对应的文件是什么，显卡还是磁盘。

最开始shell进程：/dev/tty0。

外设和内存统一编址，就用mov %ax,pos，如果是独立编址就用out。

写设备驱动就是写核心的out指令，然后注册到设备表上去。

只有一句话

完成显示中最核心的秘密就是mov pos,c。

pos指向显存：pos=0xA0000.

系统启动初始光标位置：0x90000

printf整个过程：库函数printf，系统调用write，字符设备接口crw_table，tty设备写，write_q队列，显示器写con_write。

### L27 键盘

让外设工作起来：CPU向控制器中的寄存器读写数据，控制器完成真正的工作，并向CPU发中断信号。操作系统都将设备做成文件视图。

关于键盘的故事从哪里开始？

如何使用键盘？对于使用者，敲键盘，看结果。对于操作系统：等着你敲键盘，敲了就中断。

所以故事该从键盘中断开始，从中断初始化开始。0x21中断。

中断处理函数：

```
inb $0x60, %al
call_key_table(eax)
```

调用key_table+eax*4，将字符放在一个缓冲区中，上层程序来取。

到目前为止，还差什么？回显。

总结：键盘处理。

从键盘中断处理程序开始。

## 第6部分：文件系统5讲

### L28 生磁盘的使用

仍然从硬件开始

使用磁盘从认识磁盘开始。

盘面，磁道，扇区。

磁盘的访问单位是扇区，扇区大小是512字节，扇区的大小是传输时间和碎片浪费的折衷。

#### 1. 磁盘的IO过程

磁盘的工作过程：首先将磁头移动到指定的磁道，磁道开始旋转，转到扇区后，磁生电，磁信号编程电信号，就读出来了。读到内存的缓冲区。写的时候电信号编程磁信号。读写都是移动到磁道，然后旋转移动到扇区，。移动磁头，旋转磁盘，读写。

控制器，寻道，旋转，传输。

最直接的使用磁盘：

只要往控制器中写柱面，磁头，扇区和缓存位置。只要告诉计算机这几个参数就可以了。

柱面就是磁道，磁头就是哪一个磁盘。

将这几个参数写到磁盘控制器即可。

#### 2. 盘块号读写磁盘

第一层抽象：通过盘块号读写磁盘。

磁盘驱动负责从block计算出cyl柱面，head磁头，sec扇区等参数，不用用户考虑。

问题：如何编址？为什么这样编址？

三维编址转换为一维编址。

block相邻的盘块可以快速读出。

磁盘访问时间=写入控制器时间+寻道时间12ms+旋转时间4ms+传输时间0.3ms。

主要浪费时间就是寻道时间。

相邻盘块号放相同的数据。

从CHS到扇区号，从扇区到盘块。

问题C、H、S得到的扇区号是？

```
柱面*磁头*扇区+磁头*扇区—+扇区
```

block反推回扇区号，磁头号，柱面号。

由于磁盘访问的时间主要是在寻道上，传输时间很短，因此可以一个block盘块等同于连续的几个扇区，用空间换时间的思路，虽然浪费了传输时间，但能通过一次寻道和一次旋转，读取多个扇区。

操作系统的读写磁盘的单位从一个扇区增加到一个盘块，读写的空间虽然浪费了，但磁盘读写速度变快了。磁盘空间的浪费换取了磁盘读写效率的提高。

上层读写磁盘的单位不再是扇区，而是一个盘块，连续的几个扇区。

再接着使用磁盘：程序输出block。

#### 3. 进程队列使用磁盘

第二层抽象：多个进程通过队列使用磁盘。

多个磁盘访问请求出现在请求队列怎么办？

多个进程要通过盘块号使用磁盘。

block号放到请求队列中，磁盘中断完成后，会再从请求队列中读取block号。

调度的目标是什么？调度时主要考察什么？目标当然是平均访问延迟小，寻道时间是主要矛盾。

调度算法，仍然是FCFS开始。先到先服务。

FCFS磁盘调度算法

最直观，最公平的调度。

在移动过程中把经过的请求处理了！

SSTF磁盘调度算法

shortest-seek-time First：短移动优先算法。

SSTF存在饥饿问题。

SCAN磁盘调度算法：电梯算法

SSTF：中途不回折：每个请求都有处理机会。

C-SCAN磁盘调度算法：电梯算法

SCAN+直接移动到另一端：两端请求都能很快处理。

多个进程共同使用磁盘。

放block块的时候要满足电梯队列。

生磁盘的使用整理：

进程得到盘块号，算出扇区号。

用扇区号make_req，用电梯算法add_request。

进程sleep_on

磁盘中断处理。do_hd_request算出柱面，磁头和扇区。

hd_out调用out指令完成磁盘端口读写。

如何得到盘块号，后面利用文件系统换算出来盘块号，就是完整的文件系统了。真正的磁盘管理。

### L29 从生磁盘到文件

如何从磁盘到文件的抽象。

用文件名使用磁盘更直观，而上次

如何通过文件得到盘块号。

Files-cooked disk

#### 1. 引入文件

引入文件，就是对磁盘使用的第三层抽象。

让普通用户使用raw disk许多人连扇区都不知道，要求它们根据盘块号来访问磁盘根本不可能。需要在盘块号再引入更高一层次的抽象概念，文件。

首先想想用户眼里的文件是什么样子的？

磁盘上的文件是什么样子的？

磁盘上的文件是由一堆盘块连接在一起组成的。

建立文件名字符流到盘块号集合的映射关系。

从文件得到盘块号。

映射的作用

#### 2. 连续盘块结构

第一种方式：连续盘块结构来实现文件。

文件系统映射表，就是存放文件名到盘块的对应关系。

FCB。类似于PCB。FCB只要记录文件名、起始块和盘块数量就可以了。

缺点：

test.c再增加，要覆盖别的文件时肯定不行，就要挪动这个文件，非常费时间。

文件如果动态增长就不适合了，比较适合于直接读写。

不同的文件选择不同的映射方式。

#### 3. 链式结构

链式结构也可以实现文件到盘块的映射。

FCB放在每个盘块的末尾，链接到下一个盘块。

适合动态增长，但不适合读写。读写得从第一个盘块往后找到需要的盘块。

顺序结构存取起来比较慢，动态变化快。

链式结构存取起来快，动态变化比较慢。

#### 4. 索引结构

文件实现的第三种结构：索引结构。

index inode

FCB就称之为一个node，inode就是索引结构下的FCB映射。

类似于目录。

有一个盘块号的表。有一个存储索引信息的盘块。由它来指定存储内容的所有盘块。

所有的unix的文件系统，都是基于索引结构改造的。

实际系统使用的是多级索引。

小文件是直接索引，中等文件是二阶间接索引，大文件就是三阶简介索引。

可以表示很大的文件，很小的文件高效访问，中等大小的文件访问速度也不慢。

这就是通用操作系统的魅力。

### L30 文件使用磁盘的实现

#### 1. 磁盘号的映射

核心就是字符流算出盘块号。

在fs/read_write.c中。

文件描述符，在fd中找到PCB打开文件的文件列表，每一个文件列表就是一个inode也就是FCB，然后通过inode找到数据盘块。

file_write的工作过程应该就是：

编程之前思路要清晰，思路不清晰就开始编程是完全在浪费时间。

file中一个读写指针，是开始地址fseek就是修改它，再加上count。

file_write的实现。

先找到哪些字符，再根据读写位置找到盘块号，用盘块号，buf等形成request放入电梯。

create_block算出盘块，文件抽象的核心。

如果小于7，则直接在FCB中的一个数组中存盘块。

如果block小于512，一个盘块号2个字节。

0-6直接数据块，7一重间接；8二重间接。

#### 2. 设备文件

设备文件：m_inode。

设备文件的inode。

统一的文件视图。

#### 3. 伟大的文件视图

第一条路：读写磁盘。

从文件名找到inode，根据inode找到盘块号，根据盘块号往电梯队列放，根据电梯队列的盘块号算出CHS，发往磁盘控制器，磁盘控制器的马达，电生磁，磁生电，完成读写。

第二条路：输出到显示器。

实践项目：实现proc文件。

cat /proc/psinfo文件，这些信息显然不在磁盘上，是特殊文件。在task_struct，按照文件的方式从task_struct读出来。

当inode是proc设备时，则调用proc_read读取进程信息。

inode有设备类型。

### L31 目录与文件系统

使用磁盘的最后一层抽象，从文件名到fd。

目录树。

目录树组织一堆文件。

文件，抽象一个磁盘块集合。

磁盘文件：建立了字符流到盘块集合的映射关系。

文件系统：抽象整个磁盘（第四层抽象）。

目录树。

对文件系统来说，将整个磁盘抽象成一个文件系统，

引导块，超级块，i节点，数据区。

在其它计算机上，应用结构+存储的数据可以得到那棵文件树，找到文件，读写文件。

引入目录树：

将划分后的集合再进行划分：k次划分后，每个集合中的文件树。

这一树状结构扩展性好，表示清晰，最常用。

引入了一个新的东西：目录：表示一个文件集合。

关键在目录。

实现目录就成为了关键问题。

首先需要回答：目录怎么用？

用/my/data/a定位文件a，得到文件a的FCB。

从目录树中的路径名找到FCB，就是第四层抽象。

目录中应该存什么？存放目录下所有文件的FCB吗？

问题：有什么办法（目录存什么）让系统效率更高？

存储的是文件名和FCB的编号。

要使整个系统自举，还需存一些信息。

引导块，超级块，inode位图，盘块位图，inode节点，数据区。

inode位图：哪些inode空闲，哪些被占用。

盘块位图：哪些盘块是空闲的，硬盘大小不同这个位图的大小也不同。空闲位图：表示磁盘块空闲。

超级块：记录两个位图有多大等信息。

完成全部映射下的磁盘使用：读test.c多少个字节，目录解析根目录，读入/内容找到xx，再找到test.c的inode。根据找到的FCB和file中的偏移找到盘块，把盘块加入电梯队列，从队列中取出盘块号，算出CHS，然后outp指令发给磁盘读写。

### L32 目录解析代码的实现

1，磁盘上操作系统怎么抽象出目录树的？让目录树落实到磁盘上。

将open弄明白。

get_dir完成真正的目录解析。

核心四句话正好对应目录树的四个重点：（1）root找到根目录；（2）find_entry从目录中读取目录；（3）inr是目录项中的索引节点；（4）iget再读下一层目录。

完。
